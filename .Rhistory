w[,i] <- values
rep.cnt <- rep.cnt*length(list.of.value.vectors[[i]])
}
return(w)
}
# Create an "input matrix" for e.g. a neural network or similar that
# contains all combinations of values from "mins" to "maxs" with
# steps "steps". "indices" indicates the column indices in the resulting
# matrix where the values should be inserted. "default.inputs" gives
# the values for all other inputs, which thus have constant values.
# The number of columns of the returned matrix is the bigger of the maximum
# value of "indices" or the length of "default.inputs". The number of
# rows depends on the number of possible input value combinations.
# Example: create.input.matrix(c(2,4),c(0,10),c(1,20),c(0.2,2), 0)
create.input.matrix <- function(indices, mins, maxs, steps, default.inputs=0) {
# Get number of columns desired
ncol <- max(indices, length(default.inputs))
# Create list of value vectors and get matrix with all permutations of
# them.
l <- list()
for ( i in 1:length(indices) ) {
l[[i]] <- seq(mins[i], maxs[i], steps[i])
}
pm <- create.permutation.matrix(l)
# Create matrix, initialize with default inputs and then insert values.
m <- matrix(data=default.inputs, nrow=nrow(pm), ncol=ncol, byrow=TRUE)
for ( i in 1:length(indices) ) {
m[,indices[i]] <- pm[,i]
}
return(m)
}
# Create z matrix for 3D plot, e.g. with "persp".
# mins: Vector with minimum (x,y) values, e.g. c(0,0)
# maxs: Vector with maximum (x,y) values, e.g. c(1,1)
# steps: Vector with spacing between (x,y) values, e.g. c(0.1,0.1)
# f: a function that takes an [n,2] dimension input matrix as input and gives a [n,1] output
#    (or [1,n], doesn't matter).
# Returns: z matrix with number of rows equal to length of x vector, number of columns length of y vector
# Example: x <- seq(0, 1, 0.1); y <- seq(0, 1, 0.2); z <- create.z.for.persp(x, y, rowSums); persp(x,y,z)
# Example with weighted.sum object:
# ws<-weighted.sum.new(matrix(c(0.2,0.8),ncol=1))
# x <- seq(0, 1, 0.1); y <- seq(0, 1, 0.1); z <- create.z.for.persp(x, y, ws$eval); persp(x,y,z)
# Example with nonlineardssmodel.two.inputs.new object:
# nl<-nonlineardssmodel.two.inputs.new()
# x <- seq(0, 1, 0.1); y <- seq(0, 1, 0.1); z <- create.z.for.persp(x, y, nl$eval); persp(x,y,z)
create.z.for.persp <- function(x, y, f) {
l <- list(y,x)
m <- create.permutation.matrix(l)
zvals <- f(m)
z <- matrix(zvals, nrow=length(x), ncol=length(y))
return(z)
}
# Create affine transformation "object" with transformation matrices
# "A" and "b". If "A" or "b" are NULL, then they need to be initialised
# using some other method.
affine.transformation.new <- function(A=NULL, b=NULL) {
A.matrix <- A
b.matrix <- b
m <- list(
get.A = function() { A },
get.b = function() { b },
set.A = function(value) { A <<- value },
set.b = function(value) { b <<- value },
eval = function(x) { return(A%*%x + b) }
)
class(m) <- c("AffineTranformation", class(m))
return(m)
}
# Return affine transformation "object" that scales and translates the
# coordinates from one set of ranges to another range.
scale.translate.ranges <- function(minvals, maxvals, newmins, newmaxs) {
old.ranges <- maxvals - minvals
new.ranges <- newmaxs - newmins
ratios <- new.ranges/old.ranges
A <- diag(length(minvals))*ratios
b <- (newmins - minvals)*ratios
return(affine.transformation.new(A=A, b=b))
}
# "R" implementation of Adaline, done in OOP fashion. This is
# implemented as a "sub-class" of NeuralLayer, so the only thing to
# add here is training methods.
#
# Kary FrÃ¤mling, created September 2005
#
adaline.new <- function(nbrInputs, nbrOutputs, use.trace=FALSE) {
# Set up "super" class and get its environment. We need the environment
# only if we need to access some variables directly, not only through
# the "public" interface.
# "super" has to have at least one "method" for getting the environment.
super <- neural.layer.new(nbrInputs, nbrOutputs, weighted.sum.activation, identity.output.function, use.trace)
se <- environment(super[[1]])
# Set up our "own" instance variables.
nlms <- F # Use standard LMS by default
mixtw <- 1.0 # Weight of this Adaline when used in some
#kind of "mixture model", e.g. BIMM
# Perform LMS or NLMS training. No return value.
# CAN THIS BE APPLIED TO ONLY ONE SAMPLE OR A WHOLE SET???
train <- function(t) {
targets <<- t
# Widrow-Hoff here
if ( !nlms ) {
delta <- as.vector(lr*(targets - outputs))%o%as.vector(inputs)
}
else {
nfact <- matrix(mixtw*(inputs%*%t(inputs)),
nrow=nrow(inputs), ncol=ncol(inputs))
delta <- as.vector(lr*(targets - outputs))%o%as.vector(inputs/nfact)
}
weights <<- weights + delta
}
# Perform LMS or NLMS training for given "delta" value that indicates
# the error. "delta" can be a constant.
# Should be modified so that it can also be a vector. Having a vector
# is useful for "ordinary" learning where the error value is usually
# different for every output.
# In Q-learning, the error value is global for the whole net, i.e. for
# all outputs. Then the extent to which this error is distributed to
# different outputs depends on the "eligibility trace". The
# the expression for "delta" is:
# r(t+1) + gamma*Q(s(t+1),a(t+1)) - Q(s(t),a(t))
# IMPORTANT!?? This function can only be used for discrete
# state spaces with lookup-table type of calculations. This means
# that sum(inputs^2) = 1, so NLMS only signifies using "K" factor.
train.with.delta <- function(delta) {
if ( is.null(trace) ) {
tr <- 1.0
}
else {
tr <- trace$get.trace()
}
d <- lr*delta*tr
if ( nlms ) {
nfact <- matrix(mixtw*(inputs%*%t(inputs)), nrow=nrow(d), ncol=ncol(d))
d <- d/nfact
}
weights <<- weights + d
}
# Construct list of "public methods"
pub <- list(
get.nlms = function() { nlms },
get.mixtw = function() { mixtw },
set.nlms = function(value) { nlms <<- value },
set.mixtw = function(value) { mixtw <<- value },
train.with.delta = function(delta) {
train.with.delta(delta)
},
train = function(t) { train(t) }
)
# Set up the environment so that "private" variables in "super" become
# visible. This might not always be a good choice but it is the most
# convenient here for the moment.
parent.env(environment(pub[[1]])) <- se
# We return the list of "public" methods. Since we "inherit" from
# "super" (NeuralLayer), we need to concatenate our list of "methods"
# with the inherited ones.
# Also add something to our "class". Might be useful in the future
methods <- c(pub,super)
# Also declare that we implement the "TrainableApproximator" interface
class(methods) <- c("Adaline",class(super),class(trainable.approximator.new()))
return(methods)
}
test <- function() {
a <- adaline.new(2, 3)
class(a)
print(class(a))
print(a)
print(a$get.inputs())
inputs <- c(1, 1.5)
weights <- matrix(c(0, 0, 1, 1, 2, 2), 3, 2, byrow = T)
out <- a$eval(inputs)
print(out)
a$set.weights(weights)
out <- a$eval(inputs)
print(out)
print(a$get.weights())
print(a$get.inputs())
print(a$get.outputs())
print(a$get.lr())
print(a$set.lr(0.5))
print(a$get.lr())
print(a$get.inputs())
t <- c(1, 2, 3)
a$set.nlms(T)
a$train(t)
out <- a$eval(inputs)
print(out) # Should give 0.5, 2.25, 4
print(a$get.targets()) # Should give i, 2, 3
}
#test()
m <- sombrero.data
t.in <-m[,1:2]
targets <- m[,3]
n.in <- ncol(t.in)
n.out <- 1
rbf <-
rbf.new(n.in,
n.out,
0,
activation.function = squared.distance.activation,
output.function = imqe.output.function)
rbf$set.nrbf(TRUE)
ol <- rbf$get.outlayer()
ol$set.use.bias(FALSE)
rbf$set.spread(30) # d^2 parameter in INKA
c <- 1 # The "c" parameter in INKA training, minimal distance for adding new hidden neuron.
n.hidden <-
train.inka(
rbf,
t.in,
targets,
c,
max.iter = 200,
inv.whole.set.at.end = F,
rmse.limit=0.001
)
# Calculate error measure etc.
print(n.hidden)
# Plot output surface
zvals <- rbf$eval(plot.XYvals)
z <- matrix(zvals, nrow=length(x), ncol=length(y))
persp(x, y, z)
rmse.inka <<- RMSE(test.targets,zvals)
source('~/GitHub/kfnnet/NeurIPS_2020_Framling.R')
source('~/GitHub/kfnnet/NeurIPS_2020_Framling.R')
source('~/GitHub/kfnnet/Functions.R')
source('~/GitHub/kfnnet/NeurIPS_2020_Framling.R')
source('~/GitHub/kfnnet/NeurIPS_2020_Framling.R')
source('~/GitHub/kfnnet/NeurIPS_2020_Framling.R')
source('~/GitHub/kfnnet/NeurIPS_2020_Framling.R')
source('~/GitHub/kfnnet/NeurIPS_2020_Framling.R')
source('~/GitHub/kfnnet/NeurIPS_2020_Framling.R')
udacious <- c("Chris Saden", "Lauren Castellano",
"Sarah Spikes","Dean Eckles",
"Andy Brown", "Moira Burke",
"Kunal Chawla")
udacious
numbers <- c(1:10)
numbers
numbers <- c(numbers, 11:20)
numbers
udacious <- c("Chris Saden", "Lauren Castellano",
"Sarah Spikes","Dean Eckles",
"Andy Brown", "Moira Burke",
"Kunal Chawla", YOUR_NAME)
udacious <- c("Chris Saden", "Lauren Castellano",
"Sarah Spikes","Dean Eckles",
"Andy Brown", "Moira Burke",
"Kunal Chawla", "A Z")
mystery = nchar(udacious)
mystery
mystery == 11
udacious[mystery == 11]
data(mtcars)
names(mtcars)
?mtcars
mtcars
str(mtcars)
dim(mtcars)
?row.names
row.names(mtcars)
row.names(mtcars) <- c(1:32)
mtcars
data(mtcars)
head(mtcars, 10)
head(mtcars)
tail(mtcars, 3)
mtcars$mpg
mean(mtcars$mpg)
getwd()
mtcars$wt
cond <- mtcars$wt < 3
cond
mtcars$weight_class <- ifelse(cond, 'light', 'average')
mtcars$weight_class
cond <- mtcars$wt > 3.5
mtcars$weight_class <- ifelse(cond, 'heavy', mtcars$weight_class)
mtcars$weight_class
mtcars$year <- c(1973, 1974)
mtcars
subset(mtcars, mpg >= 30 | mpg < 60)
subset(mtcars, mpg >= 30 | hp < 60)
setwd("~/GitHub/inka")
# Experiments with "sombrero" function.
library(caret)
source("rbf.R")
# Return output value for the "sombrero" function in its 3D version,
# i.e. 2 inputs, one output. The input values may be scalars, vectors,
# matrices etc.
get.sombrero.3D <- function(x, y) {
tmp <- sqrt(x^2 + y^2)
return (sin(tmp)/tmp)
}
# Get "n.samples" random (x,y) pairs from given intervals, calculate corresponding
# z values for sombrero function and return (x,y,z) matrix with n.samples rows.
# Example: m <- get.sombrero.samples(xrange=c(-10,10), yrange=c(-10,10), n.samples=100)
get.sombrero.samples <- function(xrange=c(-10,10), yrange=c(-10,10), n.samples=100) {
x <- runif(n.samples, min=xrange[1], max=xrange[2])
y <- runif(n.samples, min=yrange[1], max=yrange[2])
z <- get.sombrero.3D(x,y)
return(matrix(c(x,y,z), nrow=n.samples, byrow=FALSE))
}
# To get a 3D-plot, execute the following.
plot.sombrero.3D <- function(xrange=c(-10,10), yrange=c(-10,10), xystep=0.21, ...) {
xmin <- xrange[1]
xmax <- xrange[2]
ymin <- yrange[1]
ymax <- yrange[2]
xstep <- xystep[1]
if ( length(xystep) > 1 )
ystep <- xystep[2]
else
ystep <- xstep
xseq <- seq(xmin, xmax, xstep)
yseq <- seq(ymin, ymax, ystep)
xvals <- rep(xseq, length(yseq))
yvals <- rep(yseq, each=length(xseq))
zvals <- get.sombrero.3D(xvals, yvals)
z <- matrix(zvals, nrow=length(xseq), ncol=length(yseq))
persp(xseq, yseq, z, ...)
}
# Create training set. No noise.
sombrero.data <- get.sombrero.samples(xrange=c(-10,10), yrange=c(-10,10), n.samples=300)
trainData <- as.data.frame(sombrero.data)
names(trainData) <- c( "x", "y", "z")
# Create test&plot set.
xmin <- ymin <- -10
xmax <- ymax <- 10
xstep <- ystep <- 0.6 # Have to avoid (0,0) because it gives NaN for sombrero
# Test & plotting set
x <- seq(xmin,xmax,xstep)
y <- seq(ymin,ymax,ystep)
l <- list(y,x)
plot.XYvals <- create.permutation.matrix(l)
fxy <- as.data.frame(plot.XYvals); names(fxy) <- c("x", "y")
test.targets <- get.sombrero.3D(plot.XYvals[,1],plot.XYvals[,2])
# Do this if you want plot of function.
z <- matrix(test.targets, nrow=length(x), ncol=length(y))
persp(x, y, z)
# Testing different models from caret.
sombrero.knn <- function() {
start.time <- proc.time()        # Save starting time
logRegModel <- train(z ~ ., data=trainData, method = 'knn') # Works
#logRegModel <- train(z ~ ., data=trainData, method = 'mlp', size=100) # Not good
#logRegModel <- train(z ~ ., data=trainData, preProc = c("center", "scale"), method = 'mlpKerasDecay') # Doesn't work
#logRegModel <- train(z ~ ., data=trainData, preProc = c("center", "scale"), method = 'nnet') # Not good
#logRegPrediction <- predict(logRegModel, testData)
end.time <- proc.time()
exec.time <- end.time - start.time # Time difference between start & end
print(exec.time)
zvals <- predict(logRegModel, fxy)
z <- matrix(zvals, nrow=length(x), ncol=length(y))
persp(x, y, z)
#postResample(pred = test_set$pred, obs = test_set$obs)
rmse.knn <<- RMSE(test.targets,zvals)
}
# Use Random Forest.
sombrero.rf <- function() {
start.time <- proc.time()        # Save starting time
rfModel <- train(z ~ ., data=trainData, method = 'rf') # This works better
# library(randomForest)
# rfModel <- randomForest(z ~ .,
#                         data=trainData,
#                         importance=TRUE,
#                         ntree=5000)
end.time <- proc.time()
exec.time <- end.time - start.time # Time difference between start & end
print(exec.time)
zvals <- predict(rfModel, fxy)
z <- matrix(zvals, nrow=length(x), ncol=length(y))
persp(x, y, z)
#postResample(pred = test_set$pred, obs = test_set$obs)
rmse.rf <<- RMSE(test.targets,zvals)
}
# Test using Neural Nets package directly
sombrero.nnet <- function() {
preProcess_range_model <- preProcess(trainData, method='range')
tD <- predict(preProcess_range_model, newdata = trainData)
library(neuralnet)
#n <- names(tD)
f <- as.formula("z ~ .")
start.time <- proc.time()        # Save starting time
nn <- neuralnet(f,data=tD,hidden=c(50),linear.output=T,stepmax=1e+06, lifesign = "full")
end.time <- proc.time()
exec.time <- end.time - start.time # Time difference between start & end
print(exec.time)
pr.nn <- predict(nn,fxy)
z <- matrix(pr.nn, nrow=length(x), ncol=length(y))
persp(x, y, z)
rmse.nn <<- RMSE(test.targets,pr.nn)
}
# Make INKA training on sombrero function data and plot the resulting model.
sombrero.inka.test <- function() {
m <- sombrero.data
t.in <-m[,1:2]
targets <- m[,3]
n.in <- ncol(t.in)
n.out <- 1
rbf <-
rbf.new(n.in,
n.out,
0,
activation.function = squared.distance.activation,
output.function = imqe.output.function)
rbf$set.nrbf(TRUE)
ol <- rbf$get.outlayer()
ol$set.use.bias(FALSE)
rbf$set.spread(30) # d^2 parameter in INKA
c <- 1 # The "c" parameter in INKA training, minimal distance for adding new hidden neuron.
start.time <- proc.time()        # Save starting time
n.hidden <-
train.inka(
rbf,
t.in,
targets,
c,
max.iter = 200,
inv.whole.set.at.end = F,
rmse.limit=0.001
)
end.time <- proc.time()
exec.time <- end.time - start.time # Time difference between start & end
print(exec.time)
# Calculate error measure etc.
print(n.hidden)
# Plot output surface
zvals <- rbf$eval(plot.XYvals)
z <- matrix(zvals, nrow=length(x), ncol=length(y))
persp(x, y, z)
rmse.inka <<- RMSE(test.targets,zvals)
}
sombrero.knn
run sombrero.knn
start.time <- proc.time()        # Save starting time
logRegModel <- train(z ~ ., data=trainData, method = 'knn') # Works
#logRegModel <- train(z ~ ., data=trainData, method = 'mlp', size=100) # Not good
#logRegModel <- train(z ~ ., data=trainData, preProc = c("center", "scale"), method = 'mlpKerasDecay') # Doesn't work
#logRegModel <- train(z ~ ., data=trainData, preProc = c("center", "scale"), method = 'nnet') # Not good
#logRegPrediction <- predict(logRegModel, testData)
end.time <- proc.time()
exec.time <- end.time - start.time # Time difference between start & end
print(exec.time)
zvals <- predict(logRegModel, fxy)
z <- matrix(zvals, nrow=length(x), ncol=length(y))
persp(x, y, z)
#postResample(pred = test_set$pred, obs = test_set$obs)
rmse.knn <<- RMSE(test.targets,zvals)
start.time <- proc.time()        # Save starting time
rfModel <- train(z ~ ., data=trainData, method = 'rf') # This works better
# library(randomForest)
# rfModel <- randomForest(z ~ .,
#                         data=trainData,
#                         importance=TRUE,
#                         ntree=5000)
end.time <- proc.time()
exec.time <- end.time - start.time # Time difference between start & end
print(exec.time)
zvals <- predict(rfModel, fxy)
z <- matrix(zvals, nrow=length(x), ncol=length(y))
persp(x, y, z)
#postResample(pred = test_set$pred, obs = test_set$obs)
rmse.rf <<- RMSE(test.targets,zvals)
preProcess_range_model <- preProcess(trainData, method='range')
tD <- predict(preProcess_range_model, newdata = trainData)
library(neuralnet)
#n <- names(tD)
f <- as.formula("z ~ .")
start.time <- proc.time()        # Save starting time
nn <- neuralnet(f,data=tD,hidden=c(50),linear.output=T,stepmax=1e+06, lifesign = "full")
end.time <- proc.time()
exec.time <- end.time - start.time # Time difference between start & end
print(exec.time)
pr.nn <- predict(nn,fxy)
z <- matrix(pr.nn, nrow=length(x), ncol=length(y))
persp(x, y, z)
rmse.nn <<- RMSE(test.targets,pr.nn)
m <- sombrero.data
t.in <-m[,1:2]
targets <- m[,3]
n.in <- ncol(t.in)
n.out <- 1
rbf <-
rbf.new(n.in,
n.out,
0,
activation.function = squared.distance.activation,
output.function = imqe.output.function)
rbf$set.nrbf(TRUE)
ol <- rbf$get.outlayer()
ol$set.use.bias(FALSE)
rbf$set.spread(30) # d^2 parameter in INKA
c <- 1 # The "c" parameter in INKA training, minimal distance for adding new hidden neuron.
start.time <- proc.time()        # Save starting time
n.hidden <-
train.inka(
rbf,
t.in,
targets,
c,
max.iter = 200,
inv.whole.set.at.end = F,
rmse.limit=0.001
)
end.time <- proc.time()
exec.time <- end.time - start.time # Time difference between start & end
print(exec.time)
# Calculate error measure etc.
print(n.hidden)
# Plot output surface
zvals <- rbf$eval(plot.XYvals)
z <- matrix(zvals, nrow=length(x), ncol=length(y))
persp(x, y, z)
rmse.inka <<- RMSE(test.targets,zvals)
