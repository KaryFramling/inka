eval = function(x) { eval(x) }
)
class(m) <- c("FunctionApproximator")
return(m)
}
# Nonlinear "decision support" model for two inputs.
# Example:
nonlineardssmodel.two.inputs.new <- function() {
eval <- function(x) {
y <- x[,1]^0.5 + x[,2]^2
max <- 2 # max(y)
y <- y/max
}
m <- list (
eval = function(x) { eval(x) }
)
class(m) <- c("FunctionApproximator")
return(m)
}
# This function takes a list of vectors as parameter. Every vector
# contains a list of values. The function then creates a matrix that
# has as many rows as necessary to contain all the permutations of the
# values in the vectors. Returns the resulting matrix, which then
# has as many columns as there are vectors in the
# "list.of.value.vectors" list.
# Example: l<-list(c(1,2,3),c(4,5,6)); create.permutation.matrix(l)
create.permutation.matrix <- function(list.of.value.vectors) {
# Calculate number of rows needed
ncol <- length(list.of.value.vectors)
nrow <- 1
for ( i in 1:ncol )
nrow <- nrow*length(list.of.value.vectors[[i]])
w <- matrix(nrow=nrow, ncol=ncol)
rep.cnt <- 1
for ( i in ncol:1 ) {
values <- c()
for ( val in 1:length(list.of.value.vectors[[i]]) )
values <- c(values, rep(list.of.value.vectors[[i]][val], rep.cnt))
w[,i] <- values
rep.cnt <- rep.cnt*length(list.of.value.vectors[[i]])
}
return(w)
}
# Create an "input matrix" for e.g. a neural network or similar that
# contains all combinations of values from "mins" to "maxs" with
# steps "steps". "indices" indicates the column indices in the resulting
# matrix where the values should be inserted. "default.inputs" gives
# the values for all other inputs, which thus have constant values.
# The number of columns of the returned matrix is the bigger of the maximum
# value of "indices" or the length of "default.inputs". The number of
# rows depends on the number of possible input value combinations.
# Example: create.input.matrix(c(2,4),c(0,10),c(1,20),c(0.2,2), 0)
create.input.matrix <- function(indices, mins, maxs, steps, default.inputs=0) {
# Get number of columns desired
ncol <- max(indices, length(default.inputs))
# Create list of value vectors and get matrix with all permutations of
# them.
l <- list()
for ( i in 1:length(indices) ) {
l[[i]] <- seq(mins[i], maxs[i], steps[i])
}
pm <- create.permutation.matrix(l)
# Create matrix, initialize with default inputs and then insert values.
m <- matrix(data=default.inputs, nrow=nrow(pm), ncol=ncol, byrow=TRUE)
for ( i in 1:length(indices) ) {
m[,indices[i]] <- pm[,i]
}
return(m)
}
# Create z matrix for 3D plot, e.g. with "persp".
# mins: Vector with minimum (x,y) values, e.g. c(0,0)
# maxs: Vector with maximum (x,y) values, e.g. c(1,1)
# steps: Vector with spacing between (x,y) values, e.g. c(0.1,0.1)
# f: a function that takes an [n,2] dimension input matrix as input and gives a [n,1] output
#    (or [1,n], doesn't matter).
# Returns: z matrix with number of rows equal to length of x vector, number of columns length of y vector
# Example: x <- seq(0, 1, 0.1); y <- seq(0, 1, 0.2); z <- create.z.for.persp(x, y, rowSums); persp(x,y,z)
# Example with weighted.sum object:
# ws<-weighted.sum.new(matrix(c(0.2,0.8),ncol=1))
# x <- seq(0, 1, 0.1); y <- seq(0, 1, 0.1); z <- create.z.for.persp(x, y, ws$eval); persp(x,y,z)
# Example with nonlineardssmodel.two.inputs.new object:
# nl<-nonlineardssmodel.two.inputs.new()
# x <- seq(0, 1, 0.1); y <- seq(0, 1, 0.1); z <- create.z.for.persp(x, y, nl$eval); persp(x,y,z)
create.z.for.persp <- function(x, y, f) {
l <- list(y,x)
m <- create.permutation.matrix(l)
zvals <- f(m)
z <- matrix(zvals, nrow=length(x), ncol=length(y))
return(z)
}
# Create affine transformation "object" with transformation matrices
# "A" and "b". If "A" or "b" are NULL, then they need to be initialised
# using some other method.
affine.transformation.new <- function(A=NULL, b=NULL) {
A.matrix <- A
b.matrix <- b
m <- list(
get.A = function() { A },
get.b = function() { b },
set.A = function(value) { A <<- value },
set.b = function(value) { b <<- value },
eval = function(x) { return(A%*%x + b) }
)
class(m) <- c("AffineTranformation", class(m))
return(m)
}
# Return affine transformation "object" that scales and translates the
# coordinates from one set of ranges to another range.
scale.translate.ranges <- function(minvals, maxvals, newmins, newmaxs) {
old.ranges <- maxvals - minvals
new.ranges <- newmaxs - newmins
ratios <- new.ranges/old.ranges
A <- diag(length(minvals))*ratios
b <- (newmins - minvals)*ratios
return(affine.transformation.new(A=A, b=b))
}
# "R" implementation of Adaline, done in OOP fashion. This is
# implemented as a "sub-class" of NeuralLayer, so the only thing to
# add here is training methods.
#
# Kary FrÃ¤mling, created September 2005
#
adaline.new <- function(nbrInputs, nbrOutputs, use.trace=FALSE) {
# Set up "super" class and get its environment. We need the environment
# only if we need to access some variables directly, not only through
# the "public" interface.
# "super" has to have at least one "method" for getting the environment.
super <- neural.layer.new(nbrInputs, nbrOutputs, weighted.sum.activation, identity.output.function, use.trace)
se <- environment(super[[1]])
# Set up our "own" instance variables.
nlms <- F # Use standard LMS by default
mixtw <- 1.0 # Weight of this Adaline when used in some
#kind of "mixture model", e.g. BIMM
# Perform LMS or NLMS training. No return value.
# CAN THIS BE APPLIED TO ONLY ONE SAMPLE OR A WHOLE SET???
train <- function(t) {
targets <<- t
# Widrow-Hoff here
if ( !nlms ) {
delta <- as.vector(lr*(targets - outputs))%o%as.vector(inputs)
}
else {
nfact <- matrix(mixtw*(inputs%*%t(inputs)),
nrow=nrow(inputs), ncol=ncol(inputs))
delta <- as.vector(lr*(targets - outputs))%o%as.vector(inputs/nfact)
}
weights <<- weights + delta
}
# Perform LMS or NLMS training for given "delta" value that indicates
# the error. "delta" can be a constant.
# Should be modified so that it can also be a vector. Having a vector
# is useful for "ordinary" learning where the error value is usually
# different for every output.
# In Q-learning, the error value is global for the whole net, i.e. for
# all outputs. Then the extent to which this error is distributed to
# different outputs depends on the "eligibility trace". The
# the expression for "delta" is:
# r(t+1) + gamma*Q(s(t+1),a(t+1)) - Q(s(t),a(t))
# IMPORTANT!?? This function can only be used for discrete
# state spaces with lookup-table type of calculations. This means
# that sum(inputs^2) = 1, so NLMS only signifies using "K" factor.
train.with.delta <- function(delta) {
if ( is.null(trace) ) {
tr <- 1.0
}
else {
tr <- trace$get.trace()
}
d <- lr*delta*tr
if ( nlms ) {
nfact <- matrix(mixtw*(inputs%*%t(inputs)), nrow=nrow(d), ncol=ncol(d))
d <- d/nfact
}
weights <<- weights + d
}
# Construct list of "public methods"
pub <- list(
get.nlms = function() { nlms },
get.mixtw = function() { mixtw },
set.nlms = function(value) { nlms <<- value },
set.mixtw = function(value) { mixtw <<- value },
train.with.delta = function(delta) {
train.with.delta(delta)
},
train = function(t) { train(t) }
)
# Set up the environment so that "private" variables in "super" become
# visible. This might not always be a good choice but it is the most
# convenient here for the moment.
parent.env(environment(pub[[1]])) <- se
# We return the list of "public" methods. Since we "inherit" from
# "super" (NeuralLayer), we need to concatenate our list of "methods"
# with the inherited ones.
# Also add something to our "class". Might be useful in the future
methods <- c(pub,super)
# Also declare that we implement the "TrainableApproximator" interface
class(methods) <- c("Adaline",class(super),class(trainable.approximator.new()))
return(methods)
}
test <- function() {
a <- adaline.new(2, 3)
class(a)
print(class(a))
print(a)
print(a$get.inputs())
inputs <- c(1, 1.5)
weights <- matrix(c(0, 0, 1, 1, 2, 2), 3, 2, byrow = T)
out <- a$eval(inputs)
print(out)
a$set.weights(weights)
out <- a$eval(inputs)
print(out)
print(a$get.weights())
print(a$get.inputs())
print(a$get.outputs())
print(a$get.lr())
print(a$set.lr(0.5))
print(a$get.lr())
print(a$get.inputs())
t <- c(1, 2, 3)
a$set.nlms(T)
a$train(t)
out <- a$eval(inputs)
print(out) # Should give 0.5, 2.25, 4
print(a$get.targets()) # Should give i, 2, 3
}
#test()
m <- sombrero.data
t.in <-m[,1:2]
targets <- m[,3]
n.in <- ncol(t.in)
n.out <- 1
rbf <-
rbf.new(n.in,
n.out,
0,
activation.function = squared.distance.activation,
output.function = imqe.output.function)
rbf$set.nrbf(TRUE)
ol <- rbf$get.outlayer()
ol$set.use.bias(FALSE)
rbf$set.spread(30) # d^2 parameter in INKA
c <- 1 # The "c" parameter in INKA training, minimal distance for adding new hidden neuron.
n.hidden <-
train.inka(
rbf,
t.in,
targets,
c,
max.iter = 200,
inv.whole.set.at.end = F,
rmse.limit=0.001
)
# Calculate error measure etc.
print(n.hidden)
# Plot output surface
zvals <- rbf$eval(plot.XYvals)
z <- matrix(zvals, nrow=length(x), ncol=length(y))
persp(x, y, z)
rmse.inka <<- RMSE(test.targets,zvals)
source('~/GitHub/kfnnet/NeurIPS_2020_Framling.R')
source('~/GitHub/kfnnet/NeurIPS_2020_Framling.R')
source('~/GitHub/kfnnet/Functions.R')
source('~/GitHub/kfnnet/NeurIPS_2020_Framling.R')
source('~/GitHub/kfnnet/NeurIPS_2020_Framling.R')
source('~/GitHub/kfnnet/NeurIPS_2020_Framling.R')
source('~/GitHub/kfnnet/NeurIPS_2020_Framling.R')
source('~/GitHub/kfnnet/NeurIPS_2020_Framling.R')
source('~/GitHub/kfnnet/NeurIPS_2020_Framling.R')
udacious <- c("Chris Saden", "Lauren Castellano",
"Sarah Spikes","Dean Eckles",
"Andy Brown", "Moira Burke",
"Kunal Chawla")
udacious
numbers <- c(1:10)
numbers
numbers <- c(numbers, 11:20)
numbers
udacious <- c("Chris Saden", "Lauren Castellano",
"Sarah Spikes","Dean Eckles",
"Andy Brown", "Moira Burke",
"Kunal Chawla", YOUR_NAME)
udacious <- c("Chris Saden", "Lauren Castellano",
"Sarah Spikes","Dean Eckles",
"Andy Brown", "Moira Burke",
"Kunal Chawla", "A Z")
mystery = nchar(udacious)
mystery
mystery == 11
udacious[mystery == 11]
data(mtcars)
names(mtcars)
?mtcars
mtcars
str(mtcars)
dim(mtcars)
?row.names
row.names(mtcars)
row.names(mtcars) <- c(1:32)
mtcars
data(mtcars)
head(mtcars, 10)
head(mtcars)
tail(mtcars, 3)
mtcars$mpg
mean(mtcars$mpg)
getwd()
mtcars$wt
cond <- mtcars$wt < 3
cond
mtcars$weight_class <- ifelse(cond, 'light', 'average')
mtcars$weight_class
cond <- mtcars$wt > 3.5
mtcars$weight_class <- ifelse(cond, 'heavy', mtcars$weight_class)
mtcars$weight_class
mtcars$year <- c(1973, 1974)
mtcars
subset(mtcars, mpg >= 30 | mpg < 60)
subset(mtcars, mpg >= 30 | hp < 60)
setwd("~/GitHub/inka")
require(caret) # Contains useful functions for training/test set partitioning and similar.
source("RBF.R")
library(nnet)
library(randomForest)
#data prep
set.seed(5) # For stable results.
inTrain <- createDataPartition(y=iris$Species, p=0.75, list=FALSE) # 75% to train set
training.Iris <- iris[inTrain,]
testing.Iris <- iris[-inTrain,]
#NeuralNet
print("NeuralNet results");
irisnnpred <- nnet(Species ~ ., data=training.Iris, size=10)
nnpredicted <- predict(irisnnpred,testing.Iris,type="class")
#nnclassification <- (y == apply(y,1,max)) * 1;
#nnperf <- sum(abs(nntest.out - nnclassification)) / 2; print(nnperf) # Simple calculation of how many mis-classified
#nnpred.class <- seq(1:nrow(nnclassification)); for ( i in 1:3) { nnpred.class[nnclassification[,i]==1] <- levels(iris$Species)[i]}
#confusionMatrix(data = as.factor(nnpred.class), reference = iris[-inTrain, 5])
confusionMatrix(data = nnpredicted, reference = testing.Iris$Species)
require(caret) # Contains useful functions for training/test set partitioning and similar.
source("RBF.R")
library(nnet)
library(randomForest)
#data prep
set.seed(5) # For stable results.
inTrain <- createDataPartition(y=iris$Species, p=0.8, list=FALSE) # 75% to train set
training.Iris <- iris[inTrain,]
testing.Iris <- iris[-inTrain,]
#NeuralNet
print("NeuralNet results");
irisnnpred <- nnet(Species ~ ., data=training.Iris, size=10)
nnpredicted <- predict(irisnnpred,testing.Iris,type="class")
#nnclassification <- (y == apply(y,1,max)) * 1;
#nnperf <- sum(abs(nntest.out - nnclassification)) / 2; print(nnperf) # Simple calculation of how many mis-classified
#nnpred.class <- seq(1:nrow(nnclassification)); for ( i in 1:3) { nnpred.class[nnclassification[,i]==1] <- levels(iris$Species)[i]}
#confusionMatrix(data = as.factor(nnpred.class), reference = iris[-inTrain, 5])
confusionMatrix(data = nnpredicted, reference = testing.Iris$Species)
require(caret) # Contains useful functions for training/test set partitioning and similar.
source("RBF.R")
library(nnet)
library(randomForest)
#data prep
set.seed(5) # For stable results.
inTrain <- createDataPartition(y=iris$Species, p=0.75, list=FALSE) # 75% to train set
training.Iris <- iris[inTrain,]
testing.Iris <- iris[-inTrain,]
#NeuralNet
print("NeuralNet results");
irisnnpred <- nnet(Species ~ ., data=training.Iris, size=10)
nnpredicted <- predict(irisnnpred,testing.Iris,type="class")
nnclassification <- (nnpredicted == apply(nnpredicted,1,max)) * 1;
nnperf <- sum(abs(nntest.out - nnclassification)) / 2; print(nnperf) # Simple calculation of how many mis-classified
nnpred.class <- seq(1:nrow(nnclassification)); for ( i in 1:3) { nnpred.class[nnclassification[,i]==1] <- levels(iris$Species)[i]}
confusionMatrix(data = as.factor(nnpred.class), reference = iris[-inTrain, 5])
#INKA
print("INKA results");
rbf <- train.inka.formula(Species~., data=training.Iris, spread=0.1, max.iter=20, classification.error.limit=0)
y <- predict.rbf(rbf, newdata=testing.Iris)
classification <- (y == apply(y, 1, max)) * 1; # INKA gives "raw" output values by default.
perf <- sum(abs(test.out - classification)) / 2; print(perf) # Simple calculation of how many mis-classified
# Confusion matrix. Requires converting one-hot to factor.
pred.class <- seq(1:nrow(classification)); for ( i in 1:3) { pred.class[classification[,i]==1] <- levels(iris$Species)[i]}
confusionMatrix(data = as.factor(pred.class), reference = iris[-inTrain, 5])
#NeuralNet
print("NeuralNet results");
irisnnpred <- nnet(Species ~ ., data=training.Iris, size=10)
nnpredicted <- predict(irisnnpred,testing.Iris,type="class")
nnclassification <- (nnpredicted == apply(nnpredicted,1,max)) * 1;
#nnperf <- sum(abs(nntest.out - nnclassification)) / 2; print(nnperf) # Simple calculation of how many mis-classified
nnpred.class <- seq(1:nrow(nnclassification)); for ( i in 1:3) { nnpred.class[nnclassification[,i]==1] <- levels(iris$Species)[i]}
confusionMatrix(data = as.factor(nnpred.class), reference = iris[-inTrain, 5])
#NeuralNet
print("NeuralNet results");
irisnnpred <- nnet(Species ~ ., data=training.Iris, size=10)
nnpredicted <- predict(irisnnpred,testing.Iris,type="class")
nnclassification <- (nnpredicted == apply(as.matrix(nnpredicted),1,max)) * 1;
#nnperf <- sum(abs(nntest.out - nnclassification)) / 2; print(nnperf) # Simple calculation of how many mis-classified
nnpred.class <- seq(1:nrow(nnclassification)); for ( i in 1:3) { nnpred.class[nnclassification[,i]==1] <- levels(iris$Species)[i]}
confusionMatrix(data = as.factor(nnpred.class), reference = iris[-inTrain, 5])
#NeuralNet
print("NeuralNet results");
irisnnpred <- nnet(Species ~ ., data=training.Iris, size=10)
nnpredicted <- predict(irisnnpred,testing.Iris,type="class")
print(nnpredicted)
#INKA
print("INKA results");
rbf <- train.inka.formula(Species~., data=training.Iris, spread=0.1, max.iter=20, classification.error.limit=0)
y <- predict.rbf(rbf, newdata=testing.Iris)
print(y)
#NeuralNet
print("NeuralNet results");
irisnnpred <- nnet(Species ~ ., data=training.Iris, size=10)
nnpredicted <- predict(irisnnpred,testing.Iris,type="class")
#print(nnpredicted)
#nnclassification <- (nnpredicted == apply(nnpredicted,1,max)) * 1;
nnperf <- sum(abs(nntest.out - nnpredicted)) / 2; print(nnperf) # Simple calculation of how many mis-classified
nnpred.class <- seq(1:nrow(nnpredicted)); for ( i in 1:3) { nnpred.class[nnpredicted[,i]==1] <- levels(iris$Species)[i]}
confusionMatrix(data = as.factor(nnpred.class), reference = iris[-inTrain, 5])
print("NeuralNet results");
irisnnpred <- nnet(Species ~ ., data=training.Iris, size=10)
nnpredicted <- predict(irisnnpred,testing.Iris,type="class")
#print(nnpredicted)
#nnclassification <- (nnpredicted == apply(nnpredicted,1,max)) * 1;
#nnperf <- sum(abs(nntest.out - nnclassification)) / 2; print(nnperf) # Simple calculation of how many mis-classified
#nnpred.class <- seq(1:nrow(nnclassification)); for ( i in 1:3) { nnpred.class[nnclassification[,i]==1] <- levels(iris$Species)[i]}
confusionMatrix(data = as.factor(nnpred.class), reference = iris[-inTrain, 5])
#NeuralNet
print("NeuralNet results");
irisnnpred <- nnet(Species ~ ., data=training.Iris, size=10)
nnpredicted <- predict(irisnnpred,testing.Iris,type="class")
#print(nnpredicted)
#nnclassification <- (nnpredicted == apply(nnpredicted,1,max)) * 1;
#nnperf <- sum(abs(nntest.out - nnclassification)) / 2; print(nnperf) # Simple calculation of how many mis-classified
#nnpred.class <- seq(1:nrow(nnclassification)); for ( i in 1:3) { nnpred.class[nnclassification[,i]==1] <- levels(iris$Species)[i]}
confusionMatrix(data = as.factor(nnpredicted), reference = iris[-inTrain, 5])
#INKA
print("INKA results");
rbf <- train.inka.formula(Species~., data=training.Iris, spread=0.1, max.iter=20, classification.error.limit=0)
y <- predict.rbf(rbf, newdata=testing.Iris)
classification <- (y == apply(y, 1, max)) * 1; # INKA gives "raw" output values by default.
perf <- sum(abs(test.out - classification)) / 2; print(perf) # Simple calculation of how many mis-classified
# Confusion matrix. Requires converting one-hot to factor.
pred.class <- seq(1:nrow(classification)); for ( i in 1:3) { pred.class[classification[,i]==1] <- levels(iris$Species)[i]}
confusionMatrix(data = as.factor(pred.class), reference = iris[-inTrain, 5])
require(caret) # Contains useful functions for training/test set partitioning and similar.
source("RBF.R")
library(nnet)
library(randomForest)
#data prep
set.seed(5) # For stable results.
inTrain <- createDataPartition(y=iris$Species, p=0.75, list=FALSE) # 75% to train set
training.Iris <- iris[inTrain,]
testing.Iris <- iris[-inTrain,]
#INKA
print("INKA results");
rbf <- train.inka.formula(Species~., data=training.Iris, spread=0.1, max.iter=20, classification.error.limit=0)
y <- predict.rbf(rbf, newdata=testing.Iris)
classification <- (y == apply(y, 1, max)) * 1; # INKA gives "raw" output values by default.
perf <- sum(abs(test.out - classification)) / 2; print(perf) # Simple calculation of how many mis-classified
# Confusion matrix. Requires converting one-hot to factor.
pred.class <- seq(1:nrow(classification)); for ( i in 1:3) { pred.class[classification[,i]==1] <- levels(iris$Species)[i]}
confusionMatrix(data = as.factor(pred.class), reference = iris[-inTrain, 5])
require(caret) # Contains useful functions for training/test set partitioning and similar.
source("RBF.R")
library(nnet)
library(randomForest)
#data prep
set.seed(5) # For stable results.
inTrain <- createDataPartition(y=iris$Species, p=0.75, list=FALSE) # 75% to train set
training.Iris <- iris[inTrain,]
testing.Iris <- iris[-inTrain,]
#NeuralNet
print("NeuralNet results");
irisnnpred <- nnet(Species ~ ., data=training.Iris, size=10)
nnpredicted <- predict(irisnnpred,testing.Iris,type="class")
#print(nnpredicted)
#nnclassification <- (nnpredicted == apply(nnpredicted,1,max)) * 1;
#nnperf <- sum(abs(nntest.out - nnclassification)) / 2; print(nnperf) # Simple calculation of how many mis-classified
#nnpred.class <- seq(1:nrow(nnclassification)); for ( i in 1:3) { nnpred.class[nnclassification[,i]==1] <- levels(iris$Species)[i]}
confusionMatrix(data = as.factor(nnpredicted), reference = iris[-inTrain, 5])
require(caret) # Contains useful functions for training/test set partitioning and similar.
source("RBF.R")
library(nnet)
library(randomForest)
#data prep
set.seed(5) # For stable results.
inTrain <- createDataPartition(y=iris$Species, p=0.75, list=FALSE) # 75% to train set
training.Iris <- iris[inTrain,]
testing.Iris <- iris[-inTrain,]
#caret
print("Caret KNN Results");
knn.fit <- train(Species ~ ., data = training.Iris, method = "knn")
knn.fit
predictionknn <- predict(knn.fit, testing.Iris)
confusionMatrix(predictionknn, testing.Iris$Species)
require(caret) # Contains useful functions for training/test set partitioning and similar.
source("RBF.R")
library(nnet)
library(randomForest)
#data prep
set.seed(5) # For stable results.
inTrain <- createDataPartition(y=iris$Species, p=0.75, list=FALSE) # 75% to train set
training.Iris <- iris[inTrain,]
testing.Iris <- iris[-inTrain,]
#INKA
print("INKA results");
rbf <- train.inka.formula(Species~., data=training.Iris, spread=0.1, max.iter=20, classification.error.limit=0)
y <- predict.rbf(rbf, newdata=testing.Iris)
classification <- (y == apply(y, 1, max)) * 1; # INKA gives "raw" output values by default.
perf <- sum(abs(test.out - classification)) / 2; print(perf) # Simple calculation of how many mis-classified
# Confusion matrix. Requires converting one-hot to factor.
pred.class <- seq(1:nrow(classification)); for ( i in 1:3) { pred.class[classification[,i]==1] <- levels(iris$Species)[i]}
confusionMatrix(data = as.factor(pred.class), reference = iris[-inTrain, 5])
